{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myazdani/hacking-minGPT/blob/main/play_math.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b0pLs_DJ0uJ"
      },
      "source": [
        "## Train GPT on addition\n",
        "\n",
        "Train a GPT model on a dedicated addition dataset to see if a Transformer can learn to add."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/myazdani/hacking-minGPT.git\n",
        "\n",
        "import sys\n",
        "  \n",
        "# adding Folder_2 to the system path\n",
        "sys.path.insert(0, './hacking-minGPT/')"
      ],
      "metadata": {
        "id": "LUiKigiPJ-5j",
        "outputId": "1f1a5244-e02b-4687-88d7-b8ecc7a86f16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'hacking-minGPT'...\n",
            "remote: Enumerating objects: 54, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 54 (delta 30), reused 39 (delta 15), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (54/54), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IV956NGqJ0uM"
      },
      "outputs": [],
      "source": [
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BkEogA0qJ0uO"
      },
      "outputs": [],
      "source": [
        "# make deterministic\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NW5rjwLCJ0uO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iJS5La8RJ0uP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class AdditionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Returns addition problems of up to some number of digits in the inputs. Recall\n",
        "    that all GPT cares about are sequences of integers, and completing them according to\n",
        "    patterns in the data. Therefore, we have to somehow encode addition problems\n",
        "    as a sequence of integers.\n",
        "    \n",
        "    The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "    encoding will simply be the n-digit first number, n-digit second number, \n",
        "    and (n+1)-digit result, all simply concatenated together. Because each addition\n",
        "    problem is so structured, there is no need to bother the model with encoding\n",
        "    +, =, or other tokens. Each possible sequence has the same length, and simply\n",
        "    contains the raw digits of the addition problem.\n",
        "    \n",
        "    As a few examples, the 2-digit problems:\n",
        "    - 85 + 50 = 135 becomes the sequence [8, 5, 5, 0, 1, 3, 5]\n",
        "    - 6 + 39 = 45 becomes the sequence [0, 6, 3, 9, 0, 4, 5]\n",
        "    etc.\n",
        "    \n",
        "    We will also only train GPT on the final (n+1)-digits because the first\n",
        "    two n-digits are always assumed to be given. So when we give GPT an exam later,\n",
        "    we will e.g. feed it the sequence [0, 6, 3, 9], which encodes that we'd like\n",
        "    to add 6 + 39, and hope that the model completes the integer sequence with [0, 4, 5]\n",
        "    in 3 sequential steps.\n",
        "    \n",
        "    fun exercise: does it help if the result is asked to be produced in reverse order?\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, ndigit, split):\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        # +1 due to potential carry overflow, but then -1 because very last digit doesn't plug back\n",
        "        self.block_size = ndigit + ndigit + ndigit + 1 - 1\n",
        "        \n",
        "        # split up all addition problems into either training data or test data\n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
        "        r = np.random.RandomState(1337) # make deterministic\n",
        "        perm = r.permutation(num)\n",
        "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ixes.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to GPT and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-KkZUJEVJ0uQ"
      },
      "outputs": [],
      "source": [
        "# create a dataset for e.g. 2-digit addition\n",
        "ndigit = 2\n",
        "train_dataset = AdditionDataset(ndigit=ndigit, split='train')\n",
        "test_dataset = AdditionDataset(ndigit=ndigit, split='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gL1v-mGdJ0uR",
        "outputId": "849a4152-beb4-4089-ac72-a240f425538d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4, 7, 1, 7, 0, 6]), tensor([-100, -100, -100,    0,    6,    4]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_dataset[0] # sample a training instance just to see what one raw example looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4MNzzXGHJ0uS",
        "outputId": "b0232de2-c4e8-4086-91da-69a903786379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "07/03/2022 03:12:43 - INFO - mingpt.model -   number of parameters: 4.001280e+05\n"
          ]
        }
      ],
      "source": [
        "from mingpt.model import GPT, GPTConfig, GPT1Config\n",
        "\n",
        "# initialize a baby GPT model\n",
        "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size, \n",
        "                  n_layer=2, n_head=4, n_embd=128)\n",
        "model = GPT(mconf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HZ2AWnOBJ0uT",
        "outputId": "d95092a0-67a9-45e5-8078-3908c899f016",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "epoch 1 iter 17: train loss 1.58797. lr 5.994512e-04: 100%|██████████| 18/18 [00:00<00:00, 22.11it/s]\n",
            "07/03/2022 03:12:56 - INFO - mingpt.trainer -   test loss: 1.550834\n",
            "epoch 2 iter 17: train loss 1.35627. lr 5.977197e-04: 100%|██████████| 18/18 [00:00<00:00, 23.15it/s]\n",
            "07/03/2022 03:12:57 - INFO - mingpt.trainer -   test loss: 1.290636\n",
            "epoch 3 iter 17: train loss 1.23350. lr 5.948114e-04: 100%|██████████| 18/18 [00:00<00:00, 22.94it/s]\n",
            "07/03/2022 03:12:58 - INFO - mingpt.trainer -   test loss: 1.159823\n",
            "epoch 4 iter 17: train loss 1.11039. lr 5.907379e-04: 100%|██████████| 18/18 [00:00<00:00, 22.02it/s]\n",
            "07/03/2022 03:12:59 - INFO - mingpt.trainer -   test loss: 1.006299\n",
            "epoch 5 iter 17: train loss 0.76746. lr 5.855153e-04: 100%|██████████| 18/18 [00:01<00:00, 17.59it/s]\n",
            "07/03/2022 03:13:01 - INFO - mingpt.trainer -   test loss: 0.610585\n",
            "epoch 6 iter 17: train loss 0.61377. lr 5.791641e-04: 100%|██████████| 18/18 [00:01<00:00, 15.42it/s]\n",
            "07/03/2022 03:13:03 - INFO - mingpt.trainer -   test loss: 0.482548\n",
            "epoch 7 iter 17: train loss 0.52591. lr 5.717095e-04: 100%|██████████| 18/18 [00:00<00:00, 23.35it/s]\n",
            "07/03/2022 03:13:04 - INFO - mingpt.trainer -   test loss: 0.405616\n",
            "epoch 8 iter 17: train loss 0.49002. lr 5.631810e-04: 100%|██████████| 18/18 [00:00<00:00, 22.40it/s]\n",
            "07/03/2022 03:13:05 - INFO - mingpt.trainer -   test loss: 0.332534\n",
            "epoch 9 iter 17: train loss 0.42662. lr 5.536122e-04: 100%|██████████| 18/18 [00:00<00:00, 23.00it/s]\n",
            "07/03/2022 03:13:06 - INFO - mingpt.trainer -   test loss: 0.287562\n",
            "epoch 10 iter 17: train loss 0.39875. lr 5.430411e-04: 100%|██████████| 18/18 [00:00<00:00, 23.08it/s]\n",
            "07/03/2022 03:13:07 - INFO - mingpt.trainer -   test loss: 0.257909\n",
            "epoch 11 iter 17: train loss 0.36090. lr 5.315093e-04: 100%|██████████| 18/18 [00:00<00:00, 23.24it/s]\n",
            "07/03/2022 03:13:09 - INFO - mingpt.trainer -   test loss: 0.211305\n",
            "epoch 12 iter 17: train loss 0.33747. lr 5.190624e-04: 100%|██████████| 18/18 [00:00<00:00, 23.21it/s]\n",
            "07/03/2022 03:13:10 - INFO - mingpt.trainer -   test loss: 0.177638\n",
            "epoch 13 iter 17: train loss 0.29112. lr 5.057497e-04: 100%|██████████| 18/18 [00:00<00:00, 22.78it/s]\n",
            "07/03/2022 03:13:11 - INFO - mingpt.trainer -   test loss: 0.160488\n",
            "epoch 14 iter 17: train loss 0.30375. lr 4.916238e-04: 100%|██████████| 18/18 [00:00<00:00, 23.21it/s]\n",
            "07/03/2022 03:13:12 - INFO - mingpt.trainer -   test loss: 0.146718\n",
            "epoch 15 iter 17: train loss 0.25051. lr 4.767405e-04: 100%|██████████| 18/18 [00:00<00:00, 23.87it/s]\n",
            "07/03/2022 03:13:13 - INFO - mingpt.trainer -   test loss: 0.112667\n",
            "epoch 16 iter 17: train loss 0.22938. lr 4.611586e-04: 100%|██████████| 18/18 [00:00<00:00, 23.98it/s]\n",
            "07/03/2022 03:13:15 - INFO - mingpt.trainer -   test loss: 0.092844\n",
            "epoch 17 iter 17: train loss 0.20152. lr 4.449397e-04: 100%|██████████| 18/18 [00:00<00:00, 23.50it/s]\n",
            "07/03/2022 03:13:16 - INFO - mingpt.trainer -   test loss: 0.085457\n",
            "epoch 18 iter 17: train loss 0.22174. lr 4.281479e-04: 100%|██████████| 18/18 [00:00<00:00, 23.67it/s]\n",
            "07/03/2022 03:13:17 - INFO - mingpt.trainer -   test loss: 0.075428\n",
            "epoch 19 iter 17: train loss 0.15722. lr 4.108497e-04: 100%|██████████| 18/18 [00:00<00:00, 23.14it/s]\n",
            "07/03/2022 03:13:18 - INFO - mingpt.trainer -   test loss: 0.052712\n",
            "epoch 20 iter 17: train loss 0.14360. lr 3.931133e-04: 100%|██████████| 18/18 [00:00<00:00, 22.61it/s]\n",
            "07/03/2022 03:13:19 - INFO - mingpt.trainer -   test loss: 0.038318\n",
            "epoch 21 iter 17: train loss 0.16126. lr 3.750088e-04: 100%|██████████| 18/18 [00:00<00:00, 24.32it/s]\n",
            "07/03/2022 03:13:20 - INFO - mingpt.trainer -   test loss: 0.029170\n",
            "epoch 22 iter 17: train loss 0.12585. lr 3.566079e-04: 100%|██████████| 18/18 [00:00<00:00, 25.98it/s]\n",
            "07/03/2022 03:13:22 - INFO - mingpt.trainer -   test loss: 0.025781\n",
            "epoch 23 iter 17: train loss 0.10819. lr 3.379832e-04: 100%|██████████| 18/18 [00:00<00:00, 23.74it/s]\n",
            "07/03/2022 03:13:23 - INFO - mingpt.trainer -   test loss: 0.023226\n",
            "epoch 24 iter 17: train loss 0.12720. lr 3.192084e-04: 100%|██████████| 18/18 [00:00<00:00, 22.58it/s]\n",
            "07/03/2022 03:13:24 - INFO - mingpt.trainer -   test loss: 0.022696\n",
            "epoch 25 iter 17: train loss 0.12286. lr 3.003577e-04: 100%|██████████| 18/18 [00:00<00:00, 22.93it/s]\n",
            "07/03/2022 03:13:25 - INFO - mingpt.trainer -   test loss: 0.014049\n",
            "epoch 26 iter 17: train loss 0.09995. lr 2.815056e-04: 100%|██████████| 18/18 [00:00<00:00, 23.00it/s]\n",
            "07/03/2022 03:13:26 - INFO - mingpt.trainer -   test loss: 0.015500\n",
            "epoch 27 iter 17: train loss 0.12357. lr 2.627266e-04: 100%|██████████| 18/18 [00:00<00:00, 22.80it/s]\n",
            "07/03/2022 03:13:27 - INFO - mingpt.trainer -   test loss: 0.014815\n",
            "epoch 28 iter 17: train loss 0.09259. lr 2.440948e-04: 100%|██████████| 18/18 [00:00<00:00, 23.16it/s]\n",
            "07/03/2022 03:13:29 - INFO - mingpt.trainer -   test loss: 0.010956\n",
            "epoch 29 iter 17: train loss 0.08456. lr 2.256841e-04: 100%|██████████| 18/18 [00:00<00:00, 22.93it/s]\n",
            "07/03/2022 03:13:30 - INFO - mingpt.trainer -   test loss: 0.010215\n",
            "epoch 30 iter 17: train loss 0.06112. lr 2.075671e-04: 100%|██████████| 18/18 [00:00<00:00, 23.72it/s]\n",
            "07/03/2022 03:13:31 - INFO - mingpt.trainer -   test loss: 0.007240\n",
            "epoch 31 iter 17: train loss 0.07153. lr 1.898155e-04: 100%|██████████| 18/18 [00:00<00:00, 24.07it/s]\n",
            "07/03/2022 03:13:32 - INFO - mingpt.trainer -   test loss: 0.006970\n",
            "epoch 32 iter 17: train loss 0.07653. lr 1.724993e-04: 100%|██████████| 18/18 [00:00<00:00, 24.35it/s]\n",
            "07/03/2022 03:13:33 - INFO - mingpt.trainer -   test loss: 0.005945\n",
            "epoch 33 iter 17: train loss 0.06900. lr 1.556871e-04: 100%|██████████| 18/18 [00:00<00:00, 23.99it/s]\n",
            "07/03/2022 03:13:34 - INFO - mingpt.trainer -   test loss: 0.006250\n",
            "epoch 34 iter 17: train loss 0.06257. lr 1.394453e-04: 100%|██████████| 18/18 [00:00<00:00, 24.01it/s]\n",
            "07/03/2022 03:13:36 - INFO - mingpt.trainer -   test loss: 0.005437\n",
            "epoch 35 iter 17: train loss 0.05280. lr 1.238381e-04: 100%|██████████| 18/18 [00:00<00:00, 24.49it/s]\n",
            "07/03/2022 03:13:37 - INFO - mingpt.trainer -   test loss: 0.004936\n",
            "epoch 36 iter 17: train loss 0.09084. lr 1.089272e-04: 100%|██████████| 18/18 [00:00<00:00, 22.76it/s]\n",
            "07/03/2022 03:13:38 - INFO - mingpt.trainer -   test loss: 0.004899\n",
            "epoch 37 iter 17: train loss 0.06289. lr 9.477150e-05: 100%|██████████| 18/18 [00:00<00:00, 24.04it/s]\n",
            "07/03/2022 03:13:39 - INFO - mingpt.trainer -   test loss: 0.004143\n",
            "epoch 38 iter 17: train loss 0.04803. lr 8.142699e-05: 100%|██████████| 18/18 [00:00<00:00, 23.75it/s]\n",
            "07/03/2022 03:13:40 - INFO - mingpt.trainer -   test loss: 0.003886\n",
            "epoch 39 iter 17: train loss 0.04293. lr 6.894639e-05: 100%|██████████| 18/18 [00:00<00:00, 23.02it/s]\n",
            "07/03/2022 03:13:41 - INFO - mingpt.trainer -   test loss: 0.003714\n",
            "epoch 40 iter 17: train loss 0.04606. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 23.21it/s]\n",
            "07/03/2022 03:13:43 - INFO - mingpt.trainer -   test loss: 0.003615\n",
            "epoch 41 iter 17: train loss 0.04106. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 22.23it/s]\n",
            "07/03/2022 03:13:44 - INFO - mingpt.trainer -   test loss: 0.003393\n",
            "epoch 42 iter 17: train loss 0.03445. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 23.50it/s]\n",
            "07/03/2022 03:13:45 - INFO - mingpt.trainer -   test loss: 0.003375\n",
            "epoch 43 iter 17: train loss 0.04940. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.45it/s]\n",
            "07/03/2022 03:13:46 - INFO - mingpt.trainer -   test loss: 0.003159\n",
            "epoch 44 iter 17: train loss 0.04345. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.20it/s]\n",
            "07/03/2022 03:13:47 - INFO - mingpt.trainer -   test loss: 0.003121\n",
            "epoch 45 iter 17: train loss 0.05110. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.66it/s]\n",
            "07/03/2022 03:13:48 - INFO - mingpt.trainer -   test loss: 0.002927\n",
            "epoch 46 iter 17: train loss 0.04002. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 24.74it/s]\n",
            "07/03/2022 03:13:50 - INFO - mingpt.trainer -   test loss: 0.003100\n",
            "epoch 47 iter 17: train loss 0.04482. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 25.29it/s]\n",
            "07/03/2022 03:13:51 - INFO - mingpt.trainer -   test loss: 0.002906\n",
            "epoch 48 iter 17: train loss 0.04595. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 23.74it/s]\n",
            "07/03/2022 03:13:52 - INFO - mingpt.trainer -   test loss: 0.002860\n",
            "epoch 49 iter 17: train loss 0.05984. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 19.32it/s]\n",
            "07/03/2022 03:13:53 - INFO - mingpt.trainer -   test loss: 0.002595\n",
            "epoch 50 iter 17: train loss 0.03820. lr 6.000000e-05: 100%|██████████| 18/18 [00:00<00:00, 23.66it/s]\n",
            "07/03/2022 03:13:55 - INFO - mingpt.trainer -   test loss: 0.002710\n"
          ]
        }
      ],
      "source": [
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "\n",
        "# initialize a trainer instance and kick off training\n",
        "tconf = TrainerConfig(max_epochs=50, batch_size=512, learning_rate=6e-4,\n",
        "                      lr_decay=True, warmup_tokens=1024, final_tokens=50*len(train_dataset)*(ndigit+1),\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, test_dataset, tconf)\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kpiSd5JhJ0uT"
      },
      "outputs": [],
      "source": [
        "# now let's give the trained model an addition exam\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from mingpt.utils import sample\n",
        "\n",
        "def give_exam(dataset, batch_size=32, max_batches=-1):\n",
        "    \n",
        "    results = []\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    for b, (x, y) in enumerate(loader):\n",
        "        x = x.to(trainer.device)\n",
        "        d1d2 = x[:, :ndigit*2]\n",
        "        d1d2d3 = sample(model, d1d2, ndigit+1)\n",
        "        d3 = d1d2d3[:, -(ndigit+1):]\n",
        "        factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n",
        "        # decode the integers from individual digits\n",
        "        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n",
        "        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n",
        "        d3i_pred = (d3 * factors).sum(1)\n",
        "        d3i_gt = d1i + d2i\n",
        "        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line, lol\n",
        "        for i in range(x.size(0)):\n",
        "            results.append(int(correct[i]))\n",
        "            judge = 'YEP!!!' if correct[i] else 'NOPE'\n",
        "            if not correct[i]:\n",
        "                print(\"GPT claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
        "                      % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i], judge))\n",
        "        \n",
        "        if max_batches >= 0 and b+1 >= max_batches:\n",
        "            break\n",
        "\n",
        "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "3i5joObRJ0uU",
        "outputId": "ae44c703-5675-4ae2-e2e5-b44d57feb7e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final score: 9000/9000 = 100.00% correct\n"
          ]
        }
      ],
      "source": [
        "# training set: how well did we memorize?\n",
        "give_exam(train_dataset, batch_size=1024, max_batches=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kIbH1IBIJ0uU",
        "outputId": "3172662b-377f-4ea1-a6ef-c6169fc8c344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "final score: 1000/1000 = 100.00% correct\n"
          ]
        }
      ],
      "source": [
        "# test set: how well did we generalize?\n",
        "give_exam(test_dataset, batch_size=1024, max_batches=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "91hci28jJ0uU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "play_math.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}